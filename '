#/bin/bash
## Swift Biosciences 16S Smodule workflow
## Author Benli Chai & Sukhinder Sandhu 20200413
## Remember to edit/set the parameters in config.txt file
## Run as SModule.sh config.txt inputDir workDir
## make sure work dir exists before running the pipeline

START=$(date +%s.%N)

if [ $# -ne 4 ]
    then
         echo 'SModule.sh config.txt inputdir workdir analysisName'
         exit
fi
set -e
set -x

source $1 #load config.txt
echo `cat ${1}`
INPUTDIR=$(readlink -f $2) # the absolute dir where the fastq.gz file are located
export WD=$(readlink -f $3) #work folder
name=$4 #name of your analysis: one word
runlog='log'
current_time=$(date "+%Y.%m.%d-%H.%M.%S")
echo $current_time
log=${runlog}.${current_time}_${name}


[ ! -d "$WD" ] && echo "Directory $WD does not exist, please create one..." && exit

#make a directory for primer-trimmed sequence files
mkdir ${WD}/trimmed
cd ${WD}

##Step 1: match and trim primers from PE reads
echo -e "Matching/trimming primers...\n    Starts: $(date)" >> $log
start=$(date +%s.%N)
trimStats='trimStats.txt'
printf "SampleID\tStarting read count\tPrimer trimmed\n" >> ${trimStats}
for R1 in ${INPUTDIR}/*_R1_*fastq.gz; do
    echo $PRIMERS
    R2=${R1/_R1_/_R2_} #the path to the read 2 file
    basenameR1=${R1##*/}
    basenameR2=${R2##*/}
    prefixR1=${basenameR1%".fastq.gz"}
    prefixR2=${basenameR2%".fastq.gz"}
    prefix=${prefixR1%_L001_R1_*}
    totalCount=$(bc <<<  $(zcat ${R1}|wc -l)/4)

    #PE option to trim primers off the reads
    $CUTADAPT -e 0.10 -g file:$PRIMERS -G file:$PRIMERS \
              -o trimmed/${basenameR1} \
              -p trimmed/${basenameR2} \
              --untrimmed-output ${prefixR1}_NotTrimmed.fastq \
              --untrimmed-paired-output ${prefixR2}_NotTrimmed.fastq \
              $R1 $R2 \
              --max-n 0 \
              --minimum-length ${READLEN}

    [[ ! -s trimmed/${basenameR1} ]] \
        && rm trimmed/${basenameR1} \
        && rm trimmed/${basenameR2} \
        && echo "${prefix} zero match primers" \
        && rm ${prefix}* \
        && printf "${prefix}\t${totalCount}\t0}\n" >> ${trimStats}\
        && continue

    trimmedCount=$(bc <<<  $(zcat trimmed/${basenameR1}|wc -l)/4)
    trimPCT=$(bc <<< "scale=4 ; (${trimmedCount}/${totalCount})*100" )
    printf "${prefix}\t${totalCount}\t${trimmedCount}\n" >> ${trimStats}
    echo "$prefix trimmed: ${trimPCT}%" >> $log

done
echo "    Ends: $(date)">>$log
end=$(date +%s.%N)
runtime=$(python -c "print(${end} - ${start})")
echo "    Primer trimming Runtime: $runtime sec" >> $log

##Step 2: run Dada2 to trim read ends, obtain ASVs, and remove chimeric reads
#obtain ASVs and count table from fastq PE files of all samples
echo -e "\nRunning DADA2...\n    Starts: $(date)">>$log
start=$(date +%s.%N)
echo -e "\nDADA2 processing stats:" >> $log
#Prepare asv count tables and sequences in PEs, single formats
${SCRIPTS}/runDada2.R trimmed/ . ${READLEN} >> $log #run dada2 with R
cat "DADA2_summary.csv" | sed -e 's/"//g' >> $log
${SCRIPTS}/split_asv_count.py asv_seqNcount.csv asv

echo -e "\n    Unique ASV pairs from DADA2: $(bc <<< "$(grep -c ">" asv_seq.fasta)/2")"  >>$log
echo "    Ends: $(date)">>$log
end=$(date +%s.%N)
runtime=$(python -c "print(${end} - ${start})")
echo "    DADA2 Runtime: $runtime sec" >> $log

##Step 3:classify asv PEs with RDP classifier
echo -e "\nRunning RDP classifer...\n    Starts: $(date)">>$log
start=$(date +%s.%N)
java -Xmx1g -jar ${RDPHOME}/classifier.jar \
    -f fixrank \
    -o asv_PE.cls \
    asv_PE.fasta

end=$(date +%s.%N)
runtime=$(python -c "print(${end} - ${start})")
echo "    Read classfication Runtime: $runtime sec" >> $log

##Step 4:Data reduction, i.e. reduce the number of reads for downstream processing
#Retain only unique sequences comparing both strands
echo -e "\nDereplicating ASVs...\n    Starts: $(date)" >>$log
start=$(date +%s.%N)
$VSEARCH --cluster_size asv_seq.fasta \
         --strand both \
         --iddef 1 \
         --id 1.00 \
         --uc asv.uc \
         --centroid asv_uniq.fasta
end=$(date +%s.%N)
runtime=$(python -c "print(${end} - ${start})")
echo "    Dereplication Runtime: $runtime sec" >> $log

${SCRIPTS}/blastFilter.sh

#Determine the candiate reference sequenes, associate the sample reads,
#allocate read counts, calculate read coverage, and classify each proxy sequence
#representing each associated amplicon set.
echo -e "\nConverging candidate template sequences...\n    Starts: $(date)">>$log
start=$(date +%s.%N)
${SCRIPTS}/converge.py \
        asv_count.csv \
        asv_PE.cls \
        $log

END=$(date +%s.%N)
runtime=$(python -c "print(${END} - ${start})")
echo "    Converging Runtime: $runtime sec" >> $log
echo "\n" >> $log
runtime=$(python -c "print(${END} - ${START})")
echo -e "\nWhole process completed in: $runtime sec">>$log
